MFCCs are modeled with a Gaussian Mixture Model, which is a parametric probability density
function represented as a weighted sum of Gaussian component densities. GMMs are often
used in biometric systems, specially in speaker recognition systems, due to their
capability of representing a large class of sample distributions and one of the powerful
attributes of the GMM is its ability to form smooth aproximations to arbitrarily shaped
densities \cite{gmm_reynolds}.

The densitiy function of a GMM is defined as:

\begin{equation}
	p(x) = \sum_{k=1}^{K}\pi_{k} \mathcal{N}(x|\mu_{k},\,\Sigma_{k})
\end{equation}

Each Gaussian Density $\mathcal{N}(x|\mu_{k},\,\Sigma_{k})$ is called a component of the mixture
and has its own mean $\mu_{k}$ and variance $\Sigma_{k}$. The parameters $\pi_{k}$ are called
mixing coefficients and they can be thought as the prior probability of picking the $k^{th}$
component \cite{gmm_bishop}.

Each Gaussian component is determined by the formula:

\begin{equation}
	\mathcal{N}(x|\mu_{k},\,\Sigma_{k}) = \frac{1}{(2\pi)^{(D/2)}|\Sigma|^{1/2}} exp \big\{ -\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\}
\end{equation}

Where $D$ is the dimension of the features space (39 MFCCs in this case), the $D$-dimensional
vector $\mu$ is the mean, the $DxD$ matrix $\Sigma$ is called the covariance and $|\Sigma|$
denotes the determinant of $\Sigma$.

\subsection{Expectation Maximization Algorithm}

Given the training vectors, the objective is to estimate the parameters $\lambda$ of the GMM
that in some sense best matches the distribution of the data. The aim of the estimation is
to find the model parameters which maximize the likelihood of the given training data. For a
sequence of $N$ training vectors $X=\{x_{1}, x_{2}, \dotsc x_{N}\}$ the GMM likelihood
(assuming independence between vectors) can be written as:

\begin{equation}
	p(X|\lambda) = \prod_{n=1}^{N}p(x_{n}|\lambda)
	\label{eq:likelihoodGMM}
\end{equation}

The parameters $\lambda$ that maximizes \ref{eq:likelihoodGMM} are obtained by using an iterative
algorithm called Expectation Maximization. The basic idea of the EM algorithm is,
beginning with an initial model $\lambda$ (i.e random initialized), estimate a new model
$\bar{\lambda}$ such that $p(X|\bar{\lambda}) \geq p(X|\lambda)$. The new model then becomes
the initial model for the next iteration and the process is repeated until some convergence
model is reached.

Each iteration involves two steps that give the algorithm its name. The first step is the \emph{Expectation} step, where the posterior probabilities $\gamma(z_{nk})$ are computed.
These posteriors represents the \emph{responsibility} that component $k$ takes for
'explaining' the observation $x_{n}$.

\begin{equation}
	\gamma(z_{nk}) = \frac{\pi_{k} \mathcal{N}(x_{n}|\mu_{k},\,\Sigma_{k})}{\sum_{j=1}^{K}\pi_{k} \mathcal{N}(x_{n}|\mu_{j},\,\Sigma_{j})}
	\label{eq:expectationStep}
\end{equation}

During the Maximization step, the parameters are re-estimated using the current responsibilities:

\begin{equation}
	\mu_{k}^{new} = \frac{1}{N_{k}}\sum_{n=1}^{N}\gamma(z_{nk})x_{n}
\end{equation}

\begin{equation}
	\Sigma_{k}^{new} = \frac{1}{N_{k}}\sum_{n=1}^{N}\gamma(z_{nk})(x_{n} - \mu_{k}^{new})(x_{n} - \mu_{k}^{new})^{T}
\end{equation}

\begin{equation}
	\pi_{k}^{new} = \frac{\sum_{n=1}^{N}\gamma(z_{nk})}{N}
\end{equation}

After the Maximization step, the log likelihood of the new model is evaluated and the convergence
criterion is evaluated for either the parameters or the log likelihood:

\begin{equation}
	ln \ p(X|\mu, \Sigma, \pi) = \sum_{n=1}^{N}ln\Big\{\sum_{k=1}^{K}\pi_{k}\mathcal{N}(x_{n}|\mu_{k},\,\Sigma_{k})\Big\}
\end{equation}


If the convergence criterion is not satisfied, the algorithm returns to \ref{eq:expectationStep}
to start a new iteration of the Expectation and Maximization steps.

% In order to generate the Gaussian Mixture Model from our data, the Expectation Maximization
% Algorithm is used.

% \subsubsection{Definition and explanation of GMM}
% 	Given a particular phoneme, Normal Distributions are used to model the distribution of each class (correct
% 	 and mispronounced), where the space of the random variable consists in the MFCCs extracted from each instance.

% 	Instead of using a single Gaussian for each class, a Mixture is used because it allows
% 	a much richer representation of a density model. The optimal number of Gaussians used for each
% 	mixture is assumed to be proportional to the number of instances used to train the model.
% 	Because of the differences in the number of instances for each phoneme, and also for each
% 	class given a particular phoneme, each mixture is composed of a different number of gaussians.

% \subsubsection{Log Likelihood Ratio method to asses pronunciation}
% 	The core idea is based on the fact that the pattern of features of mispronounced instances are most likely to be found in the Gaussian Mixture of the incorrect instances than in the mixture of the
% 	correct ones. Of course, the opposite scenario occurs when considering the well pronounced
% 	instances.

% 	Given a sample, its class is predicted by calculating
% 	the log likelihood ratio between the chances of the instances of being mispronounced and
% 	the chances of being correct. An instance is considered as incorrect by the system whenever
% 	the LLR is above 0, and in the other way as correct when the LLR is below 0.

% \subsubsection{GMM adaptation method}
% 	Given a particular phoneme, the Gaussian Mixture Model for each class is computed by adaptation.
% 	An intial GMM is trained with all the instances, and it will be used as basis for the next steps. This kind of model trained with all the instances and intended to be the basis of future
% 	adaptation is named \textit{Universal Background Model} (UBM).
% 	From there, two GMMs are created by adapting both weights and means of the base Gaussians according to a \textit{relevance factor} parameter and the number of features used in the adaptation. The more instances are used in the process, the more confidence will be perceived
% 	over the new values thus allowing a more aggressive adaptation.

% \subsubsection{Generating supervectors from adapted GMMs}
% 	In the next experiments, the adaptation technique is used again but for a different purpose.
% 	The task consist in generating input features for a \textit{SVM} classifier. In order to do that,
% 	the first step is training an \textit{UBM} for a particular phoneme with all the
% 	instances, both positives and negatives. After that, for each instance a GMM is obtained
% 	by adapting the UBM to the new MFCCs. Usually, the parameters adaptation comprises a trade off
% 	between the previous means and weights and the new values obtained. In this case,
% 	the most aggressive strategy is used, because the previous values are not being taken into
% 	account.

% 	The final step in the process is the supervector generation, that is obtained by a concatenation
% 	of all the means of the adapted gmm and their respectives weights.
