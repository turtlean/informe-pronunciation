In this section we describe the chosen features for the current study. The features can be classified in two different categories: frame-level features and
phone-level features.

Frame-level features are computed directly from the speech signal, and they carry spectral density information. For the current work, the standard Mel Frequency Cepstral Coefficients
were chosen.

The frame-level features are then used as base to compute more complex features:
The phone-level features. These
are the features used to train the SVM and classify the instances. In the current work, two
types of phone-level features are studied: \textit{Supervectors}
which are obtained from an adaptation process of
a Gaussian Mixture Model trained on the MFCCs and the \textit{Dynamic Features},
which are
the coefficients obtained from different parameterization methods of the MFCCs values through time.
One of the main objectives of this work is to find out whether or not there is a gain in performance
when combining the Supervectors with the Dynamic Features.

In the following subsections a detailed description of each of the aforementioned features is
provided.

\subsection{Frame-level Features: MFCCs}

The first step in order to build the system is to extract acoustic features from the speech data.
The feature extraction process aims at approximating the linguistic content that is conveyed
by the speech signal, by identifying relevant aspects such as information about
the frequencies involved during the speech, and discarding unuseful properties.

In this work we base our features in the standard and widely used Mel Frecuency
Cepstral Coefficients (MFCCs). They were introduced by Davis and Mermelstein in 1980 and
have been state-of-the-art ever since \cite{mfcc_foundational}.
MFCC coefficients represent the spectral envelope of the speech signal on the Mel-frequency scale,
which relates the perceived frequency of a pure tune to its actual measured frequency inspired
by the human hearing.

The process to compute the MFCCs coefficients can be summarized by the following steps:

\begin{enumerate}

  \item Preemphasis: Preemphasis aims at increasing the amplitude of high frequency bands while
  decreasing the amplitudes of lower bands by means of a high-pass filter. This stage is performed
  in order to balance the frequency spectrum since high frequencies usually have smaller
  magnitudes compared to lower ones.
  % http://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html
  % https://www.quora.com/Why-is-pre-emphasis-i-e-passing-the-speech-signal-through-a-first-order-high-pass-filter-required-in-speech-processing-and-how-does-it-work

  \item Windowing: The MFCCs are computed in time intervals where the audio signal is not
  supposed to be changing
  so much. Because of that reason the signal is divided in frames using a
  25 ms duration window and 10 ms shift (standard values). A 25 ms duration window provides
  enough samples to get a reliable spectral estimate: 200 samples per frame at 8 $kHz$,
  and at the same time is short enough to minimize the signal changes.
  On the other hand, a 10 ms shift leads to a
  15 ms overlap between consecutive frames, ensuring that the information between adjacent
  frames is also captured in the middle of another frame.
  % http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/

  \item Discrete Fourier Transform: This is the first and main step in regard to spectral
  analysis. Frequency domain approaches are proven to be
  among the best options in speech classification tasks.
  The \textit{spectral density}, which is the distribution of power
  of the frequencies composing the signal, is obtained for each frame by using
  the Discrete Fourier Transform technique.   % mfcc_extraction_v3

  \item Mel Filter-bank: At this stage, a perceptual scale of pitches inspired in human
  hearing called Mel scale is used. Because humans are much better at discerning small changes
  in pitch at low frequencies than they are at high frequencies,
  the Mel frequency scale is linear up to 1000Hz and logarithmic thereafter according to
  the following formula:

  \begin{equation}
    M(f)=1125*ln(1 + \frac{f}{700})
  \end{equation}

  The spectrum obtained in (3) is passed through a series of traingular filters
  uniformly spaced on the Mel scale to produce the so called filter-bank energies.
  The original values of these energies are replaced by their natural logarithm values,
  also motivated by human hearing, because loudness is not perceived according to
  a linear but an exponential scale.

  \item Cepstral Coefficients: The Discrete Cosine Transform (DCT) is applied to the logarithm
  of the filterbank energies to obtain the Cepstral Coefficients. In general, only the lower
  12 Cepstral Coefficients are kept. The resulting features are called Mel Frequency
  Cepstral Coefficients. Additionally, a $13\textsuperscript{th}$ MFCC computed
  as the sum of the energy in the frame is included,
  because it usually improves the performance in phone detection
  related tasks.

  \item Deltas and Double Deltas: Finally, the speech also carries information
  in the dynamics, i.e, trajectories of the MFCC coefficients over time. Calculating
  the MFCC trajectories and appending them to the original MFCC vector usually increase the
  performance of the systems that are based on MFCCs.
  Both Delta and Delta-Delta features are included, adding to
  a total of 39 features per frame.

\end{enumerate}

Even though the frame-level features are made up of different features: 13 MFCCs plus 13 deltas
and 13 double-deltas, for the sake of simplicity the expression ``MFCCs''
will be referring to the whole 39 features throughout the current work.
