In order to analyse if
there is a statistically significant gain in combining the
supervectors and the dynamic features
over using supervectors as the only features,
two different techiques are used in this work: \textit{Bootstrapping} and
\textit{McNemar's Test}.

Bootstrapping is a technique that models the underlying
distribution by using a resampling method on the available samples while McNemar's
Test is an statistical test used on paired data.

An advantage for McNemar's
over Bootstrapping is that it bases its comparisons on cross information using
exactly the same sample.
McNemar's Test however assumes that the samples
are independent, which is not true when considering
all the instances of a given phone. Even though Bootstrapping also assumes that
the samples are independent, in the current work resampling is performed
at speaker level, which ensures the independence of the instances being resampled.
For this reason, we use both tests, since both have advantages and
disadvantages with respect to the other one.

\subsection{Bootstrapping} \label{subsection:bootstrapping}

\textit{Bootstrapping} \cite{bootstrapping} is a general intuitive method applicable
to almost any kind of sample statistic and can be understood without much
knowledge of sampling distributions. It was introduced in 1979 by B. Efron and it has
been widely used in numerous areas since then.

Supposing that it were possible to draw repeated samples
of the same size from the population of interest a large number of times, then
a fairly good idea about the sampling distribution
of a particular statistic could be obtained from the collection of the
values arising from these repeated samples.
% of a particular statistic from the collection of its
% values arising from these repeated samples could be obtained.
This method, however, does not make sense in practice since it assumes one has access to the
underlying distribution.
The idea behind \textit{Bootstrapping} is to use the data of a sample study at hand as a
``surrogate population'' for the purpose of approximating the sampling distribution of
a statistic. This is achieved by resampling with replacement from the sample data
at hand, creating a large number of alternative or duplicated sample sets known as
bootstrap samples. The process is usually repeated between 1 and 10 thousand
times, and the performance metric, EER in our case,
is computed on each of the bootstrap
samples.
% (usually between 1-10 thousand iterations).
A histogram of these computed
values is referred to as the bootstrap distribution of the statistic.

In the current work, resampling is performed at speaker level.
In other words, given a set $S$ of $n$ speakers in the
test set, in each iteration of bootstrapping
a new set of $n$ speakers $S\textprime$ is obtained by drawing speakers with replacement from the
original set $S$. The sample set is then obtained by gathering the instances of the speakers
in $S\textprime$, which will include repeated samples for the speakers that appears more than once
in $S\textprime$.
\textit{Bootstrapping} technique is used along with Confidence
Intervals in order to determine if the SVM trained with the combined features
performs better than the one trained only with features derived from the GMM adaptation.
The first step is to generate a distribution of EER for both types of classifiers.
After that,
a 95\% confidence interval is computed for each distribution by finding the interval
[$\hat{\theta}_{B1}, \hat{\theta}_{B2}$] that enclose the 95\% of the samples
composing each distribution.
If it is true that the SVM trained with
the combined features performs better than the
SVM trained with a single feature source, then its confidence
interval should be shifted with respect to the confidence interval
of the latter system.
The bigger the overlap between both intervals, the
lesser the evidence of the results coming from different distributions, thus
resulting in less significant results.
